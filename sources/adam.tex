\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsopn}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage[colorlinks=true, urlcolor=black, citecolor=black]{hyperref}    
\usepackage{cleveref}
\usepackage{float}
\usepackage{pgffor}
\usepackage{epstopdf}
\usepackage{tikz}
\usepackage{venndiagram}
\usepackage{amsthm}
\usepackage{algorithmic}
\usepackage{comment}
\ifpdf
  \DeclareGraphicsExtensions{.eps,.pdf,.png,.jpg}
\else
  \DeclareGraphicsExtensions{.eps}
\fi
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\DeclareMathOperator{\diag}{diag}

\begin{document}

\title{Literature Notes}

\author{\IEEEauthorblockN{Adam Lehavi}
\IEEEauthorblockA{\textit{Viterbi School of Engineering} \\
\textit{University of Southern California}\\
Los Angeles, CA 90089 \\
alehavi@usc.edu}}

\maketitle

\begin{abstract}
This outlines Adam Lehavi's notes related to his research with Yoonsoo Nam.
\end{abstract}

\section{Working Model Idea}
Take a video, use \cite{4604096} to turn the video into key frames, use image summarization as surveyed in \cite{bernardi2016automatic} to generate image summaries, and use an NLP model to turn a series of image summaries to a description.

\section{Core Definitions and Acronyms}
\begin{itemize}
    \item RNN - Recurrent Neural Network
    \item mRNN - multimodal RNN
    \item RCNN - Region Convolutional Neural Network
    \item BRNN - Bidirectional RNN
    \item Semantics - referring to the arrangement and relation of words in sentence formation
\end{itemize}

\section{Literature Review}
\subsection{Survey of Techniques for Labeling Video, Audio, and Text Data}

Past approaches to video data:
\begin{enumerate}
    \item Automatically annotate news videos through unsupervised learning using mining of similar videos. For a given video with a speech-recognized transcript, it first searches and ranks most similar videos and then mines those.
    \item Automatically annotate of people passing surveillance cameras, annotating clothing color, height, and focus of attention.
    \item Use TREC video retrieval evaluation.
    \item Address the problem of automatic temporal annotation of realistic human actions in video.
    \item Use an optimal graph (OGL) from multicues (partial tags and multiple features) to get results superior to other state-of-the-art methods.
    \item Propose Interactive Self-Annotation framework, based on recurrent self-supervised learning.
    \item State a recursive and semi-automatic annotation approach which proposes initial annotations for all frames in a video based on segmenting only a few manual objects.
    \item Use Multiple Annotation Maturation (MAM)
    \item Propose a novel Correlative Multi-Labe (CML) framework which simultaneously classifies concepts and models correlations in a single step on the TRECVID dataset
\end{enumerate}
\cite{DBLP:journals/corr/abs-2109-03784}

Note a lot of the papers are from the ACM international conference on Multimedia.

\subsection{Video Summarization Based on ML}
Process to summarize a video in terms of some smaller amount of frames/images \cite{4604096}

\subsection{Survey for Automatic Description Generation from Images}
Datasets
\begin{itemize}
    \item Pascal1K
    \item Flickr8K
    \item Flickr30K
    \item MS COCO
\end{itemize}
Recommendation is MS COCO, having 164K images each with 5 texts, collected judgements, and partial objects.

To read:
\begin{enumerate}
    \item Karpathy and Fei-Fei (2015) MultRetrieval on Flickr8K/30K, COCO measured w/ BLEU, Meteor, CIDEr, mRank, R@k
    \item Jia et al. (2015) Generation on Flickr8K/30K, COCO measured w/ BLEU, Meteor
    \item Yagcioglu et al. (2015) VisRetrieval on Flikr8K/30K, COCO measured w/ Human, BLEU, Meteor, CIDEr
\end{enumerate}
\cite{bernardi2016automatic}

\subsection{Deep Visual-Semantic Alignments for Generating Image Descriptions}

Model inputs an image, and generates a space of objects and their regions which is then used to generate a description. Trained by having images and descriptions, and aligning objects in the description to regions. This data is then used to train a mRNN to generate the snippets.

Sentences written by people make frequent references to some particular but unknown location in the image.

Images detected with a RCNN pretrained on ImageNet and its Detection Challenge.

Word representations computed using BRNN.

Overall good results.

\includegraphics[width=\linewidth]{images/karpathy1.PNG}
\includegraphics[width=\linewidth]{images/karpathy2.PNG}

\cite{karpathy2015deep}

\subsection{Guiding LSTM for Image Caption Generation}
Use a CNN for encoding and an LSTM for the decoding step, but with some guiding factor, leading to what they attempt to coin as a gLSTM.

Hard-Attention seems to perform better on COCO.
\includegraphics[width=\linewidth]{images/jia1.PNG}

\cite{jia2015guiding}

\subsection{Deep Learning for Video Captioning}

Video $V$ gets decomposed to frames, as well as motion, audio, and semantics. Sentence $Y$ gets decomposed to words, so feature extraction is as such:
\begin{align}
    V &= \{f_1,f_2,\ldots,f_N\} \\
    Y &= \{y_1,y_2,\ldots,y_T\} \\
    \mathcal{F} &= \{F_V,F_M,F_A,F_S\} \\
    \mathcal{F} &= f_{feat}(V) \\
    F_t &= f_{aggr}(\mathcal{F},s_t)
\end{align}

Possible idea is to replace $V$ with key frames from \cite{4604096} and see if using this can improve training / results.

Extraction:
\begin{itemize}
    \item Visual: CNNs like VGG and Inception Networks
    \item Motion: 3D CNNs from spatiotemporal features
    \item Audio: Mel Frequency Cepstral Coefficients (MFCC) used to get features, and bag-of-audio-words acquire fixed length audio features
    \item Semantic: video-level category information
\end{itemize}

Aggregation:
\begin{itemize}
    \item LSTM/GRU is simplest since modalities have variate length.
    \item Temporal Attention: hLSTM
    \item Spatial Attention: MAM-RNN considers prior frames for spatial attention and SAM distinguishes foreground and background
    \item Multimodal Feature Fusion: Rarely explored, MMVD, AttenstionFusion, and MA-LSTM
\end{itemize}

Scoring:
\begin{itemize}
    \item BLEU: used in past, not differentiable
    \item CIDEr: based on n-grams like BLEU
    \item SPICE: shown to generate better captions when used with CIDEr
\end{itemize}

Datasets:
\begin{itemize}
    \item MSR-VTT contains 10,000 clips from 20 categories, with the most descriptions and largest vocab size. Recommended.
\end{itemize}

\includegraphics[width=\linewidth]{images/chen1.PNG}
\cite{chen2019deep}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references}

\end{document}
